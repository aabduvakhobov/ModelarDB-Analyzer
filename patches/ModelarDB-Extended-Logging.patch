diff --git a/build.sbt b/build.sbt
index cb8001d..53ddc19 100644
--- a/build.sbt
+++ b/build.sbt
@@ -12,9 +12,9 @@ libraryDependencies ++= Seq(
 
   /* Query Engine */
   "com.h2database" % "h2" % "1.4.200",
-  "org.apache.spark" %% "spark-core" % "3.2.1" % "provided",
-  "org.apache.spark" %% "spark-streaming" % "3.2.1" % "provided",
-  "org.apache.spark" %% "spark-sql" % "3.2.1" % "provided",
+  "org.apache.spark" %% "spark-core" % "3.2.1",
+  "org.apache.spark" %% "spark-streaming" % "3.2.1",
+  "org.apache.spark" %% "spark-sql" % "3.2.1",
 
   /* Storage Layer */
   //H2 is a full RDBMS with both a query engine and a storage layer
diff --git a/src/main/java/dk/aau/modelardb/core/utility/Logger.java b/src/main/java/dk/aau/modelardb/core/utility/Logger.java
index 3381e8b..7d06914 100644
--- a/src/main/java/dk/aau/modelardb/core/utility/Logger.java
+++ b/src/main/java/dk/aau/modelardb/core/utility/Logger.java
@@ -21,6 +21,7 @@ import dk.aau.modelardb.core.models.ModelType;
 import java.io.IOException;
 import java.io.Serializable;
 import java.util.Map;
+import java.util.logging.*;
 
 public class Logger implements Serializable {
 
@@ -108,12 +109,30 @@ public class Logger implements Serializable {
 
         long finalizedCounter = this.finalizedSegmentCounter.values().stream().mapToLong(Long::longValue).sum();
         sb.append("\nFinalized Segment Counter - Total: ").append(finalizedCounter).append('\n');
+        if (!this.finalizedSegmentCounter.containsKey("dk.aau.modelardb.core.models.PMC_MeanModelType")){
+            sb.append("-- ").append("dk.aau.modelardb.core.models.PMC_MeanModelType").append(" | Count: ").append("0").append('\n');
+        }
+        if (!this.finalizedSegmentCounter.containsKey("dk.aau.modelardb.core.models.SwingFilterModelType")){
+            sb.append("-- ").append("dk.aau.modelardb.core.models.SwingFilterModelType").append(" | Count: ").append("0").append('\n');
+        }
+        if (!this.finalizedSegmentCounter.containsKey("dk.aau.modelardb.core.models.FacebookGorillaModelType")) {
+            sb.append("-- ").append("dk.aau.modelardb.core.models.FacebookGorillaModelType").append(" | Count: ").append("0").append('\n');
+        }
         for (Map.Entry<String, Long> e : this.finalizedSegmentCounter.entrySet()) {
             sb.append("-- ").append(e.getKey()).append(" | Count: ").append(e.getValue()).append('\n');
         }
 
         finalizedCounter = this.finalizedDataPointCounter.values().stream().mapToLong(Long::longValue).sum();
         sb.append("\nFinalized Segment DataPoint Counter - Total: ").append(finalizedCounter).append('\n');
+        if (!this.finalizedDataPointCounter.containsKey("dk.aau.modelardb.core.models.PMC_MeanModelType")){
+            sb.append("-- ").append("dk.aau.modelardb.core.models.PMC_MeanModelType").append(" | DataPoint: ").append("0").append('\n');
+        }
+        if (!this.finalizedDataPointCounter.containsKey("dk.aau.modelardb.core.models.SwingFilterModelType")){
+            sb.append("-- ").append("dk.aau.modelardb.core.models.SwingFilterModelType").append(" | DataPoint: ").append("0").append('\n');
+        }
+        if (!this.finalizedDataPointCounter.containsKey("dk.aau.modelardb.core.models.FacebookGorillaModelType")){
+            sb.append("-- ").append("dk.aau.modelardb.core.models.FacebookGorillaModelType").append(" | DataPoint: ").append("0").append('\n');
+        }
         for (Map.Entry<String, Long> e : this.finalizedDataPointCounter.entrySet()) {
             sb.append("-- ").append(e.getKey()).append(" | DataPoint: ").append(e.getValue()).append('\n');
         }
@@ -124,6 +143,9 @@ public class Logger implements Serializable {
         sb.append("\nCompression Ratio: ").append((16.0 * finalizedCounter) / finalizedTotalSize).append('\n');
         sb.append("---------------------------------------------------------");
         System.out.println(sb);
+
+        java.util.logging.Logger logger = getJobLogger();
+        logger.info(sb.toString());
     }
 
     public void printWorkingSetResult() {
@@ -143,9 +165,49 @@ public class Logger implements Serializable {
         System.out.println("---------------------------------------------------------");
         printAlignedDebugVariables("Total Size", getTotalSize(), cs);
         System.out.println("---------------------------------------------------------");
+
+        java.util.logging.Logger logger = getJobLogger();
+        StringBuilder sb = new StringBuilder("Time: " + getTimeSpan());
+        sb.append("\nSegments: " + finalizedSegmentCounter);
+        sb.append("\nData Points: " + finalizedPointCounter);
+        sb.append(stringAlignedDebugVariables("\nData Points Size", finalizedPointCounter * 16.0F, cs));
+        sb.append(stringAlignedDebugVariables("\nMetadata Size", this.finalizedMetadataSize, cs));
+        sb.append(stringAlignedDebugVariables("\nModels Size", this.finalizedModelsSize, cs));
+        sb.append(stringAlignedDebugVariables("\nGaps Size", this.finalizedGapsSize, cs));
+        sb.append(stringAlignedDebugVariables("\nTotal Size", getTotalSize(), cs));
+        logger.info(sb.toString());
     }
 
     /** Private Methods **/
+    private java.util.logging.Logger getJobLogger() {
+        if(jobLogger == null) {
+            jobLogger =  java.util.logging.Logger.getLogger("ResultLogs");
+            try {
+                fh = new FileHandler("./IngestionLog.log");
+                fh.setFormatter(new SimpleFormatter());
+                jobLogger.addHandler(fh);
+            } catch (SecurityException e) {
+                throw new RuntimeException(e);
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+            jobLogger.setLevel(Level.INFO);
+        }
+        return jobLogger;
+    }
+
+
+    private String stringAlignedDebugVariables(String variableName, double sizeInBytes, int cs) {
+        StringBuilder sb = new StringBuilder();
+        sb.append(String.format("%16s: %" + cs + ".0f B | %" + cs + ".3f KB | %" + cs + ".3f MB",
+                variableName,
+                sizeInBytes,
+                sizeInBytes / 1024.0F,
+                sizeInBytes / 1024.0F / 1024.0F));
+        return sb.toString();
+    }
+
+
     private void printAlignedDebugVariables(String variableName, double sizeInBytes, int cs) {
         System.out.format("%16s: %" + cs + ".0f B | %" + cs + ".3f KB | %" + cs + ".3f MB\n",
                 variableName,
@@ -176,4 +238,7 @@ public class Logger implements Serializable {
 
     private final java.util.HashMap<String, Long> finalizedSegmentCounter = new java.util.HashMap<>();
     private final java.util.HashMap<String, Long> finalizedDataPointCounter = new java.util.HashMap<>();
+
+    private FileHandler fh;
+    private static java.util.logging.Logger jobLogger = null;
 }
diff --git a/src/main/scala/dk/aau/modelardb/engines/h2/H2.scala b/src/main/scala/dk/aau/modelardb/engines/h2/H2.scala
index 0d93028..dd89bb4 100644
--- a/src/main/scala/dk/aau/modelardb/engines/h2/H2.scala
+++ b/src/main/scala/dk/aau/modelardb/engines/h2/H2.scala
@@ -19,14 +19,14 @@ import dk.aau.modelardb.core._
 import dk.aau.modelardb.core.utility.{Logger, SegmentFunction, Static}
 import dk.aau.modelardb.engines.{EngineUtilities, QueryEngine}
 import dk.aau.modelardb.remote.{ArrowResultSet, QueryInterface, RemoteStorageFlightProducer, RemoteUtilities}
-
 import org.h2.expression.condition.{Comparison, ConditionAndOr, ConditionInConstantSet}
 import org.h2.expression.{Expression, ExpressionColumn, ValueExpression}
 import org.h2.table.TableFilter
 import org.h2.value.{ValueInt, ValueTimestamp}
-
 import org.apache.arrow.flight.{FlightServer, Location}
-import org.codehaus.jackson.map.util.ISO8601Utils
+import org.apache.hadoop.shaded.org.codehaus.jackson.map.util.ISO8601Utils
+//import org.codehaus.jackson.map.util.ISO8601Utils
+//import com.fasterxml.jackson.databind.util.ISO8601Utils
 
 import java.sql.{DriverManager, Timestamp}
 import java.util.concurrent.{CountDownLatch, Executors}
diff --git a/src/main/scala/dk/aau/modelardb/engines/spark/Spark.scala b/src/main/scala/dk/aau/modelardb/engines/spark/Spark.scala
index e550849..631d737 100644
--- a/src/main/scala/dk/aau/modelardb/engines/spark/Spark.scala
+++ b/src/main/scala/dk/aau/modelardb/engines/spark/Spark.scala
@@ -46,12 +46,12 @@ class Spark(configuration: Configuration, sparkStorage: SparkStorage) extends Qu
     this.sparkSession = ss
 
     //Starts listening for and executes queries using the user-configured interface
-    QueryInterface.start(configuration, this)
+    //QueryInterface.start(configuration, this)
 
     //Ensures that Spark does not terminate until ingestion is safely stopped
     if (ssc != null) {
       Static.info("ModelarDB: awaiting termination")
-      ssc.stop(false, true)
+//      ssc.stop(false, true)
       ssc.awaitTermination()
       Spark.getCache.flush()
     }
@@ -179,7 +179,16 @@ class Spark(configuration: Configuration, sparkStorage: SparkStorage) extends Qu
       stream.foreachRDD(Spark.getCache.update(_))
       Static.info("ModelarDB: Spark Streaming initialized in online-analytics mode")
     }
-
+    var counter = 0
+    stream.foreachRDD(rdd => {
+      if (rdd.isEmpty()) counter += 1
+      else if (!rdd.isEmpty() & counter > 0) counter -= 1
+      if (counter == 10) {
+        Spark.getCache.flush()
+        println(s"EVALUATION TOOL: Empty RDDS reached ${counter}. Terminating ingestion")
+        ssc.stop()
+      }
+    })
     //The streaming context is started and the context is returned
     ssc.start()
     ssc
